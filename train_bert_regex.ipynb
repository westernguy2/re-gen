{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_bert_regex.ipynb","provenance":[{"file_id":"https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb","timestamp":1638302238928}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"e67Ut53QYEdU","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1638424483601,"user_tz":480,"elapsed":331,"user":{"displayName":"Kunal Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2IyyPs0gtDZ0y5efw94mocEVIc6hbCkoEp2F4=s64","userId":"16336361098390008573"}},"outputId":"718a9d71-0864-4752-8839-613b054baf8f"},"source":["#@title\n","%%html\n","<div style=\"background-color: pink;\">\n","  Notebook written in collaboration with <a href=\"https://github.com/aditya-malte\">Aditya Malte</a>.\n","  <br>\n","  The Notebook is on GitHub, so contributions are more than welcome.\n","</div>\n","<br>\n","<div style=\"background-color: yellow;\">\n","  Aditya wrote another notebook with a slightly different use case and methodology, please check it out.\n","  <br>\n","  <a target=\"_blank\" href=\"https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\">\n","    https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\n","  </a>\n","</div>\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"background-color: pink;\">\n","  Notebook written in collaboration with <a href=\"https://github.com/aditya-malte\">Aditya Malte</a>.\n","  <br>\n","  The Notebook is on GitHub, so contributions are more than welcome.\n","</div>\n","<br>\n","<div style=\"background-color: yellow;\">\n","  Aditya wrote another notebook with a slightly different use case and methodology, please check it out.\n","  <br>\n","  <a target=\"_blank\" href=\"https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\">\n","    https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\n","  </a>\n","</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"M1oqh0F6W3ad"},"source":["# How to train a new language model from scratch using Transformers and Tokenizers\n","\n","### Notebook edition (link to blogpost [link](https://huggingface.co/blog/how-to-train)). Last update May 15, 2020\n","\n","\n","Over the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.\n","\n","In this post we’ll demo how to train a “small” model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT – on **Esperanto**. We’ll then fine-tune the model on a downstream task of part-of-speech tagging.\n"]},{"cell_type":"markdown","metadata":{"id":"oK7PPVm2XBgr"},"source":["## 1. Find a dataset\n","\n","First, let us find a corpus of text in Esperanto. Here we’ll use the Esperanto portion of the [OSCAR corpus](https://traces1.inria.fr/oscar/) from INRIA.\n","OSCAR is a huge multilingual corpus obtained by language classification and filtering of [Common Crawl](https://commoncrawl.org/) dumps of the Web.\n","\n","<img src=\"https://huggingface.co/blog/assets/01_how-to-train/oscar.png\" style=\"margin: auto; display: block; width: 260px;\">\n","\n","The Esperanto portion of the dataset is only 299M, so we’ll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.\n","\n","The final training corpus has a size of 3 GB, which is still small – for your model, you will get better results the more data you can get to pretrain on. \n","\n"]},{"cell_type":"code","metadata":{"id":"HOk4iZ9YZvec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638424492563,"user_tz":480,"elapsed":5505,"user":{"displayName":"Kunal Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2IyyPs0gtDZ0y5efw94mocEVIc6hbCkoEp2F4=s64","userId":"16336361098390008573"}},"outputId":"1825e0c0-a94e-49e2-e660-b7fa8a833ba4"},"source":["# in this notebook we'll only get one of the files (the Oscar one) for the sake of simplicity and performance\n","!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-02 05:54:47--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n","Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 54.192.18.43, 54.192.18.58, 54.192.18.90, ...\n","Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|54.192.18.43|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 312733741 (298M) [text/plain]\n","Saving to: ‘oscar.eo.txt’\n","\n","oscar.eo.txt        100%[===================>] 298.25M  92.0MB/s    in 3.4s    \n","\n","2021-12-02 05:54:50 (87.7 MB/s) - ‘oscar.eo.txt’ saved [312733741/312733741]\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IZtlOcRY7heF","executionInfo":{"status":"ok","timestamp":1638838120624,"user_tz":480,"elapsed":24248,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"c6b5aca4-5c98-4f2f-8ac1-1f899f5f5a0f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"CpcDXru8wVh0","executionInfo":{"status":"ok","timestamp":1638838136965,"user_tz":480,"elapsed":1460,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["# use this cell to create data\n","import pandas as pd\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/ANLP21/data/regex_data/labeled_data_huge.csv\")\n","df['regex'].to_csv('/content/drive/MyDrive/ANLP21/data/regex_data/regex_only_huge.txt', sep=' ', header=False, index=False)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G-kkz81OY6xH"},"source":["## 2. Train a tokenizer\n","\n","We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s arbitrarily pick its size to be 52,000.\n","\n","We recommend training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"]},{"cell_type":"code","metadata":{"id":"5duRggBRZKvP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838183048,"user_tz":480,"elapsed":46085,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"9f153ead-5ac5-472a-9dd5-676635994f35"},"source":["# We won't need TensorFlow here\n","!pip uninstall -y tensorflow\n","# Install `transformers` from master\n","!pip install git+https://github.com/huggingface/transformers\n","!pip list | grep -E 'transformers|tokenizers'\n","# transformers version at notebook update --- 2.11.0\n","# tokenizers version at notebook update --- 0.8.0rc1"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.7.0\n","Uninstalling tensorflow-2.7.0:\n","  Successfully uninstalled tensorflow-2.7.0\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-aky2xuf5\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-aky2xuf5\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 469 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 10.6 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 35.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.4.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 48.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.1.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3256376 sha256=a74b7767dfe0ada240285bbb085052cfc03e4690451c41d3e2d7e5dc5aea1351\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-yt3uzhuo/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0.dev0\n","tokenizers                    0.10.3\n","transformers                  4.13.0.dev0\n"]}]},{"cell_type":"code","metadata":{"id":"-jy6vOQLE1pO","executionInfo":{"status":"ok","timestamp":1638838183049,"user_tz":480,"elapsed":4,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["reberth_path = \"/content/drive/MyDrive/ANLP21/ReBERTh\"\n","data_path = \"/content/drive/MyDrive/ANLP21/data/regex_data\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMnymRDLe0hi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838184066,"user_tz":480,"elapsed":1020,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"ea9c0a5a-f740-4deb-d39d-c30d04583205"},"source":["%%time \n","from pathlib import Path\n","\n","from tokenizers import ByteLevelBPETokenizer\n","\n","# paths = [str(x) for x in Path(\"/content/drive/Starred/ANLP21/data/regex_data/final_data.txt\")]\n","paths = [\"/content/drive/MyDrive/ANLP21/data/regex_data/regex_only_huge.txt\"]\n","print(paths)\n","\n","# Initialize a tokenizer\n","tokenizer = ByteLevelBPETokenizer()\n","\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=1_000, min_frequency=2, special_tokens=[\n","    \"<s>\",\n","    \"<pad>\",\n","    \"</s>\",\n","    \"<unk>\",\n","    \"<mask>\",\n","])"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/MyDrive/ANLP21/data/regex_data/regex_only_huge.txt']\n","CPU times: user 1.75 s, sys: 12.8 ms, total: 1.77 s\n","Wall time: 973 ms\n"]}]},{"cell_type":"markdown","metadata":{"id":"6Ei7bqpRf1LH"},"source":["Now let's save files to disk"]},{"cell_type":"code","metadata":{"id":"KLBZaW2bJVF0","executionInfo":{"status":"ok","timestamp":1638838187228,"user_tz":480,"elapsed":3164,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["model_sizes = ['mini', 'small', 'medium', 'base']\n","for size in model_sizes:\n","  %mkdir -p $reberth_path/$size\n","  tokenizer.save_model(f\"{reberth_path}/{size}\")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lOOfYSuQhSqT"},"source":["🔥🔥 Wow, that was fast! ⚡️🔥\n","\n","We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n","\n","```json\n","{\n","\t\"<s>\": 0,\n","\t\"<pad>\": 1,\n","\t\"</s>\": 2,\n","\t\"<unk>\": 3,\n","\t\"<mask>\": 4,\n","\t\"!\": 5,\n","\t\"\\\"\": 6,\n","\t\"#\": 7,\n","\t\"$\": 8,\n","\t\"%\": 9,\n","\t\"&\": 10,\n","\t\"'\": 11,\n","\t\"(\": 12,\n","\t\")\": 13,\n","\t# ...\n","}\n","\n","# merges.txt\n","l a\n","Ġ k\n","o n\n","Ġ la\n","t a\n","Ġ e\n","Ġ d\n","Ġ p\n","# ...\n","```\n","\n","What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto – `ĉ`, `ĝ`, `ĥ`, `ĵ`, `ŝ`, and `ŭ` – are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\n","\n","Here’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it directly from `transformers`.\n"]},{"cell_type":"code","metadata":{"id":"tKVWB8WShT-z","executionInfo":{"status":"ok","timestamp":1638838187771,"user_tz":480,"elapsed":139,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["from tokenizers.implementations import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","\n","tokenizer = ByteLevelBPETokenizer(\n","    f\"{reberth_path}/vocab.json\",\n","    f\"{reberth_path}/merges.txt\",\n",")"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"hO5M3vrAhcuj","executionInfo":{"status":"ok","timestamp":1638838187771,"user_tz":480,"elapsed":7,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n","tokenizer.enable_truncation(max_length=512)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3Ye27nchfzq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838187771,"user_tz":480,"elapsed":6,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"53d1fea3-02b5-4962-9fcc-69c07be13d44"},"source":["tokenizer.encode(\"(dog)\")"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"X8ya5_7rhjKS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838187772,"user_tz":480,"elapsed":6,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"4454f3b8-e5a8-4a82-ce35-200689066d09"},"source":["tokenizer.encode(\"(dog)\").tokens"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>', '(', 'dog', ')', '</s>']"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"WQpUC_CDhnWW"},"source":["## 3. Train a language model from scratch\n","\n","**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n","\n","> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n","\n","As the model is BERT-like, we’ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n"]},{"cell_type":"code","metadata":{"id":"kD140sFjh0LQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838187952,"user_tz":480,"elapsed":184,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"a29442db-03f1-450d-8afb-a0cb2e53d1c3"},"source":["# Check that we have a GPU\n","!nvidia-smi"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Dec  7 00:49:47 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   70C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"VNZZs-r6iKAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838193591,"user_tz":480,"elapsed":5640,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"4507c13a-9984-41fa-9ebf-dc9e098c982d"},"source":["# Check that PyTorch sees it\n","import torch\n","torch.cuda.is_available()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"u0qQzgrBi1OX"},"source":["### We'll define the following config for the model"]},{"cell_type":"code","metadata":{"id":"LTXXutqeDzPi","executionInfo":{"status":"ok","timestamp":1638838194408,"user_tz":480,"elapsed":359,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["from transformers import RobertaConfig\n","\n","mini_config = RobertaConfig(\n","    vocab_size=1_000,\n","    max_position_embeddings=512,\n","    hidden_size=256,\n","    num_attention_heads=4,\n","    num_hidden_layers=4,\n","    type_vocab_size=1,\n",")\n","\n","\n","small_config = RobertaConfig(\n","    vocab_size=1_000,\n","    max_position_embeddings=512,\n","    hidden_size=512,\n","    num_attention_heads=8,\n","    num_hidden_layers=4,\n","    type_vocab_size=1,\n",")\n","\n","medium_config = RobertaConfig(\n","    vocab_size=1_000,\n","    max_position_embeddings=512,\n","    hidden_size=512,\n","    num_attention_heads=8,\n","    num_hidden_layers=8,\n","    type_vocab_size=1,\n",")\n","\n","base_config = RobertaConfig(\n","    vocab_size=1_000,\n","    max_position_embeddings=512,\n","    hidden_size=768,\n","    num_attention_heads=12,\n","    num_hidden_layers=12,\n","    type_vocab_size=1,\n",")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwJjEGWuG2P2","executionInfo":{"status":"ok","timestamp":1638838194409,"user_tz":480,"elapsed":3,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["configs = {\n","    'mini': mini_config,\n","    'small': small_config,\n","    'medium': medium_config,\n","    'base': base_config\n","}"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yAwQ82JiE5pi"},"source":["Now let's re-create our tokenizer in transformers"]},{"cell_type":"code","metadata":{"id":"4keFBUjQFOD1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838197333,"user_tz":480,"elapsed":2926,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"451692c3-1ccb-447e-f2e2-1f0e48416297"},"source":["from transformers import RobertaTokenizerFast\n","\n","# tokenizer = RobertaTokenizerFast.from_pretrained(reberth_path, max_len=512)\n","tokenizers = {k: RobertaTokenizerFast.from_pretrained(f\"{reberth_path}/{k}\", max_len=512) for k in configs.keys()}"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["file /content/drive/MyDrive/ANLP21/ReBERTh/small/config.json not found\n","file /content/drive/MyDrive/ANLP21/ReBERTh/small/config.json not found\n","file /content/drive/MyDrive/ANLP21/ReBERTh/medium/config.json not found\n","file /content/drive/MyDrive/ANLP21/ReBERTh/medium/config.json not found\n","file /content/drive/MyDrive/ANLP21/ReBERTh/base/config.json not found\n","file /content/drive/MyDrive/ANLP21/ReBERTh/base/config.json not found\n"]}]},{"cell_type":"markdown","metadata":{"id":"6yNCw-3hFv9h"},"source":["Finally let's initialize our model.\n","\n","**Important:**\n","\n","As we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint."]},{"cell_type":"code","metadata":{"id":"BzMqR-dzF4Ro","executionInfo":{"status":"ok","timestamp":1638838212453,"user_tz":480,"elapsed":4242,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["from transformers import RobertaForMaskedLM\n","\n","models = {k: RobertaForMaskedLM(config=config) for k, config in configs.items()} "],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"jU6JhBSTKiaM"},"source":["# model.num_parameters()\n","# => 84 million parameters"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jBtUHRMliOLM"},"source":["### Now let's build our training Dataset\n","\n","We'll build our dataset by applying our tokenizer to our text file.\n","\n","Here, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box."]},{"cell_type":"code","metadata":{"id":"GlvP_A-THEEl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838229837,"user_tz":480,"elapsed":12331,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"e39cd2ee-8b77-4571-ac11-f209cd598439"},"source":["%%time\n","from transformers import LineByLineTextDataset\n","\n","datasets = {k: LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=\"/content/drive/MyDrive/ANLP21/data/regex_data/regex_only_huge.txt\",\n","    block_size=128,\n",") for k, tokenizer in tokenizers.items()}"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 16.4 s, sys: 480 ms, total: 16.9 s\n","Wall time: 12.1 s\n"]}]},{"cell_type":"code","metadata":{"id":"bbJFLIOut7B1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638838229837,"user_tz":480,"elapsed":4,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"2d4fff62-bb66-40e8-b475-ec02e9068d4f"},"source":["datasets"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'base': <transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x7f91adef5950>,\n"," 'medium': <transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x7f9187ed0a90>,\n"," 'mini': <transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x7f91adcd0650>,\n"," 'small': <transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x7f918f367210>}"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"hDLs73HcIHk5"},"source":["Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n","\n","This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on."]},{"cell_type":"code","metadata":{"id":"zTgWPa9Dipk2","executionInfo":{"status":"ok","timestamp":1638838229838,"user_tz":480,"elapsed":4,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = {k: DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",") for k, tokenizer in tokenizers.items()}"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ri2BIQKqjfHm"},"source":["### Finally, we are all set to initialize our Trainer"]},{"cell_type":"code","metadata":{"id":"YpvnFFmZJD-N","executionInfo":{"status":"ok","timestamp":1638838239572,"user_tz":480,"elapsed":9737,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}}},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = {k: TrainingArguments(\n","    output_dir=f\"{reberth_path}/{k}\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,\n","    per_gpu_train_batch_size=64,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    prediction_loss_only=True,\n",") for k in configs.keys()}\n","\n","trainers = {k: Trainer(\n","    model=model,\n","    args=training_args[k],\n","    data_collator=data_collator[k],\n","    train_dataset=datasets[k],\n",") for k, model in models.items()}"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6sASa36Nf-N"},"source":["### Start training"]},{"cell_type":"code","metadata":{"id":"Tds8o6r0K7uq","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"ok","timestamp":1638844476155,"user_tz":480,"elapsed":538031,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"c713c61d-06ca-4c61-f2b2-17f6cf99009e"},"source":["%%time\n","trainers['mini'].train()"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","***** Running training *****\n","  Num examples = 68034\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5320\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5320' max='5320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5320/5320 08:57, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>4.085000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.667500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>2.133600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.820900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.672200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.561900</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.475900</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.447400</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.398100</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.389500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 8min 39s, sys: 19.1 s, total: 8min 58s\n","Wall time: 8min 58s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=5320, training_loss=1.9297516786962523, metrics={'train_runtime': 537.9436, 'train_samples_per_second': 632.353, 'train_steps_per_second': 9.89, 'total_flos': 299743328226048.0, 'train_loss': 1.9297516786962523, 'epoch': 5.0})"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"kJHy0wyNK7-H","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"ok","timestamp":1638843937828,"user_tz":480,"elapsed":850743,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"49202386-335e-418c-fd06-a61e61907af6"},"source":["%%time\n","trainers['small'].train()"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","***** Running training *****\n","  Num examples = 68034\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5320\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5320' max='5320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5320/5320 14:10, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.088500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.743000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.434500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.288100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.196800</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.136900</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.084700</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.039200</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.001700</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.980300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 13min 51s, sys: 21.9 s, total: 14min 13s\n","Wall time: 14min 10s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=5320, training_loss=1.3728270566553102, metrics={'train_runtime': 850.6099, 'train_samples_per_second': 399.913, 'train_steps_per_second': 6.254, 'total_flos': 688928135161440.0, 'train_loss': 1.3728270566553102, 'epoch': 5.0})"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"eAhzCUZIK8lz","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"ok","timestamp":1638843086462,"user_tz":480,"elapsed":1546714,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"00b5f659-dfa3-4f1f-bf65-cf42f915301e"},"source":["%%time\n","trainers['medium'].train()"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","***** Running training *****\n","  Num examples = 68034\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5320\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5320' max='5320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5320/5320 25:46, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.935900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.647000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.368700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.208500</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.095300</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.001000</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.926800</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.872700</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.839400</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.809500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 25min 24s, sys: 28.3 s, total: 25min 52s\n","Wall time: 25min 46s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=5320, training_loss=1.242900928698088, metrics={'train_runtime': 1546.5432, 'train_samples_per_second': 219.955, 'train_steps_per_second': 3.44, 'total_flos': 1365872174603424.0, 'train_loss': 1.242900928698088, 'epoch': 5.0})"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"VmaHZXzmkNtJ","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"ok","timestamp":1638841537750,"user_tz":480,"elapsed":3298182,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"ebe965d9-e66f-462f-ea75-e8066cccb173"},"source":["%%time\n","trainers['base'].train()"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","***** Running training *****\n","  Num examples = 68034\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5320\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5320' max='5320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5320/5320 54:57, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.532500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.420900</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.166700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.003500</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.843400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.744300</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.653900</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.600100</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.563800</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.529000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 51min 33s, sys: 3min 36s, total: 55min 9s\n","Wall time: 54min 58s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=5320, training_loss=0.9756975446428572, metrics={'train_runtime': 3298.0518, 'train_samples_per_second': 103.143, 'train_steps_per_second': 1.613, 'total_flos': 3455386280661024.0, 'train_loss': 0.9756975446428572, 'epoch': 5.0})"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"_ZkooHz1-_2h"},"source":["#### 🎉 Save final model (+ tokenizer + config) to disk"]},{"cell_type":"code","metadata":{"id":"2zjf77KCLSeB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638844477205,"user_tz":480,"elapsed":1060,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"813afeb3-8a28-484f-e43d-873220b98acc"},"source":["trainers['mini'].save_model(f\"{reberth_path}/mini\")"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/ANLP21/ReBERTh/mini\n","Configuration saved in /content/drive/MyDrive/ANLP21/ReBERTh/mini/config.json\n","Model weights saved in /content/drive/MyDrive/ANLP21/ReBERTh/mini/pytorch_model.bin\n"]}]},{"cell_type":"code","metadata":{"id":"2nk4oeNELSu_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638843938127,"user_tz":480,"elapsed":309,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"0c552556-e968-4e4f-94c1-593ad311aa13"},"source":["trainers['small'].save_model(f\"{reberth_path}/small\")"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/ANLP21/ReBERTh/small\n","Configuration saved in /content/drive/MyDrive/ANLP21/ReBERTh/small/config.json\n","Model weights saved in /content/drive/MyDrive/ANLP21/ReBERTh/small/pytorch_model.bin\n"]}]},{"cell_type":"code","metadata":{"id":"FKDmfNHHLS-k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638843087088,"user_tz":480,"elapsed":636,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"e091ed4c-958a-4466-b56a-f09ae82b4e26"},"source":["trainers['medium'].save_model(f\"{reberth_path}/medium\")"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/ANLP21/ReBERTh/medium\n","Configuration saved in /content/drive/MyDrive/ANLP21/ReBERTh/medium/config.json\n","Model weights saved in /content/drive/MyDrive/ANLP21/ReBERTh/medium/pytorch_model.bin\n"]}]},{"cell_type":"code","metadata":{"id":"ZjZE46EcLT-k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638841539752,"user_tz":480,"elapsed":1641,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"db37b53b-bf8b-407e-bb41-1e4b795c1a72"},"source":["trainers['base'].save_model(f\"{reberth_path}/base\")"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/ANLP21/ReBERTh/base\n","Configuration saved in /content/drive/MyDrive/ANLP21/ReBERTh/base/config.json\n","Model weights saved in /content/drive/MyDrive/ANLP21/ReBERTh/base/pytorch_model.bin\n"]}]},{"cell_type":"code","metadata":{"id":"QDNgPls7_l13","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638397802649,"user_tz":480,"elapsed":1027,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"8aba64e0-090c-45a9-d6ab-599ea8ebb74e"},"source":["trainer.save_model(reberth_path)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/ANLP21/ReBERTh\n","Configuration saved in /content/drive/MyDrive/ANLP21/ReBERTh/config.json\n","Model weights saved in /content/drive/MyDrive/ANLP21/ReBERTh/pytorch_model.bin\n"]}]},{"cell_type":"markdown","metadata":{"id":"d0caceCy_p1-"},"source":["## 4. Check that the LM actually trained"]},{"cell_type":"markdown","metadata":{"id":"iIQJ8ND_AEhl"},"source":["Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n","\n","Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n","\n"]},{"cell_type":"code","metadata":{"id":"ltXgXyCbAJLY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638397831457,"user_tz":480,"elapsed":1047,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"bba85ca7-130b-46bb-be4c-145661003c1f"},"source":["from transformers import pipeline\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=reberth_path,\n","    tokenizer=reberth_path\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/ANLP21/ReBERTh/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/ANLP21/ReBERTh\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 1000\n","}\n","\n","loading configuration file /content/drive/MyDrive/ANLP21/ReBERTh/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/ANLP21/ReBERTh\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 1000\n","}\n","\n","loading weights file /content/drive/MyDrive/ANLP21/ReBERTh/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForMaskedLM.\n","\n","All the weights of RobertaForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/ANLP21/ReBERTh.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file /content/drive/MyDrive/ANLP21/ReBERTh/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/ANLP21/ReBERTh\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 1000\n","}\n","\n","Didn't find file /content/drive/MyDrive/ANLP21/ReBERTh/tokenizer.json. We won't load it.\n","Didn't find file /content/drive/MyDrive/ANLP21/ReBERTh/added_tokens.json. We won't load it.\n","Didn't find file /content/drive/MyDrive/ANLP21/ReBERTh/special_tokens_map.json. We won't load it.\n","Didn't find file /content/drive/MyDrive/ANLP21/ReBERTh/tokenizer_config.json. We won't load it.\n","loading file /content/drive/MyDrive/ANLP21/ReBERTh/vocab.json\n","loading file /content/drive/MyDrive/ANLP21/ReBERTh/merges.txt\n","loading file None\n","loading file None\n","loading file None\n","loading file None\n","loading configuration file /content/drive/MyDrive/ANLP21/ReBERTh/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/ANLP21/ReBERTh\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 1000\n","}\n","\n","loading configuration file /content/drive/MyDrive/ANLP21/ReBERTh/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/ANLP21/ReBERTh\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.13.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 1000\n","}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"UIvgZ3S6AO0z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638397834178,"user_tz":480,"elapsed":283,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"0c3a3b02-f21d-4b64-e580-935efb1fa2b9"},"source":["# The sun <mask>.\n","# =>\n","\n","fill_mask(\"(dog<mask>\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.12063060700893402,\n","  'sequence': '(dog).*',\n","  'token': 264,\n","  'token_str': ').*'},\n"," {'score': 0.051579393446445465,\n","  'sequence': '(dog))',\n","  'token': 304,\n","  'token_str': '))'},\n"," {'score': 0.026141837239265442,\n","  'sequence': '(dog)).*',\n","  'token': 320,\n","  'token_str': ')).*'},\n"," {'score': 0.020209401845932007,\n","  'sequence': '(dog.*',\n","  'token': 261,\n","  'token_str': '.*'},\n"," {'score': 0.01259597111493349,\n","  'sequence': '(dog){',\n","  'token': 311,\n","  'token_str': '){'}]"]},"metadata":{},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"i0qCyyhNAWZi"},"source":["Ok, simple syntax/grammar works. Let’s try a slightly more interesting prompt:\n","\n"]},{"cell_type":"code","metadata":{"id":"YZ9HSQxAAbme","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638397839062,"user_tz":480,"elapsed":275,"user":{"displayName":"Parth Baokar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTr0F0GxdJiWND7WqOvJ5YHc3aW3a2XD5sEKvS=s64","userId":"03945953594447876238"}},"outputId":"c77046f7-754d-4109-d855-9cf51c520157"},"source":["fill_mask(\"Jen la komenco de bela <mask>.\")\n","\n","# This is the beginning of a beautiful <mask>.\n","# =>"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.3046925365924835,\n","  'sequence': 'Jen la komenco de beladog.',\n","  'token': 266,\n","  'token_str': 'dog'},\n"," {'score': 0.13810423016548157,\n","  'sequence': 'Jen la komenco de belab.',\n","  'token': 70,\n","  'token_str': 'b'},\n"," {'score': 0.08295250684022903,\n","  'sequence': 'Jen la komenco de belatruck.',\n","  'token': 289,\n","  'token_str': 'truck'},\n"," {'score': 0.06642985343933105,\n","  'sequence': 'Jen la komenco de belaz.',\n","  'token': 94,\n","  'token_str': 'z'},\n"," {'score': 0.058431822806596756,\n","  'sequence': 'Jen la komenco de belaAEIOUaeiou.',\n","  'token': 279,\n","  'token_str': 'AEIOUaeiou'}]"]},"metadata":{},"execution_count":88}]},{"cell_type":"markdown","metadata":{"id":"6RsGaD1qAfLP"},"source":["## 5. Share your model 🎉"]},{"cell_type":"markdown","metadata":{"id":"5oESe8djApQw"},"source":["Finally, when you have a nice model, please think about sharing it with the community:\n","\n","- upload your model using the CLI: `transformers-cli upload`\n","- write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:\n","    - a model description,\n","    - training params (dataset, preprocessing, hyperparameters), \n","    - evaluation results,\n","    - intended uses & limitations\n","    - whatever else is helpful! 🤓\n","\n","### **TADA!**\n","\n","➡️ Your model has a page on http://huggingface.co/models and everyone can load it using `AutoModel.from_pretrained(\"username/model_name\")`.\n","\n","[![tb](https://huggingface.co/blog/assets/01_how-to-train/model_page.png)](https://huggingface.co/julien-c/EsperBERTo-small)\n"]},{"cell_type":"markdown","metadata":{"id":"aw9ifsgqBI2o"},"source":["If you want to take a look at models in different languages, check https://huggingface.co/models\n","\n","[![all models](https://huggingface.co/front/thumbnails/models.png)](https://huggingface.co/models)\n"]}]}